{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaja-asm/cry-detection/blob/main/tf_lite_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgb5S3nbJXKL",
        "outputId": "1e24a85a-0213-4fd2-ade3-6602758d1ff7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scipy.ndimage import zoom\n",
        "import ctypes\n",
        "\n",
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#     except RuntimeError as e:\n",
        "#         print(e)\n",
        "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YClY_ATkJXKP"
      },
      "outputs": [],
      "source": [
        "AUDIO_PATH = 'CryCorpusFinal'\n",
        "CRY_FOLDER = os.path.join(AUDIO_PATH, 'cry/augmented')\n",
        "NOTCRY_FOLDER = os.path.join(AUDIO_PATH, 'notcry')\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5QaLHaEJXKQ"
      },
      "outputs": [],
      "source": [
        "def load_audio_files(folder):\n",
        "    files = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.wav'):\n",
        "            files.append(os.path.join(folder, filename))\n",
        "    return files\n",
        "\n",
        "def compute_spectrogram(y, sr, n_fft=2048, hop_length=512):\n",
        "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
        "    D_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "    return D_dB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upjbPHe6JXKR"
      },
      "outputs": [],
      "source": [
        "def save_spectrogram_to_disk(D_dB, save_path):\n",
        "    if not os.path.exists(os.path.dirname(save_path)):\n",
        "        os.makedirs(os.path.dirname(save_path))\n",
        "    np.save(save_path, D_dB)\n",
        "\n",
        "cry_files = load_audio_files(CRY_FOLDER)\n",
        "notcry_files = load_audio_files(NOTCRY_FOLDER)\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for idx, file in enumerate(cry_files):\n",
        "    y, sr = librosa.load(file, sr=None)\n",
        "    y = librosa.util.normalize(y)\n",
        "    D_dB = compute_spectrogram(y, sr)\n",
        "    save_path = os.path.join(f'{0}/spectrograms'.format(AUDIO_PATH), f'cry_{idx}.npy')\n",
        "    save_spectrogram_to_disk(D_dB, save_path)\n",
        "    data.append(save_path)\n",
        "    labels.append(1)\n",
        "\n",
        "for idx, file in enumerate(notcry_files):\n",
        "    y, sr = librosa.load(file, sr=None)\n",
        "    y = librosa.util.normalize(y)\n",
        "    D_dB = compute_spectrogram(y, sr)\n",
        "    save_path = os.path.join(f'{0}/spectrograms'.format(AUDIO_PATH), f'notcry_{idx}.npy')\n",
        "    save_spectrogram_to_disk(D_dB, save_path)\n",
        "    data.append(save_path)\n",
        "    labels.append(0)\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuPLsd8VJXKU",
        "outputId": "8e77312c-5488-4a5c-d1f1-a121040d0fc9"
      },
      "outputs": [],
      "source": [
        "# Split the datasets\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "class OnTheFlyDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, file_paths, labels, batch_size, img_size, shuffle=True, augment=False):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(self.file_paths))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_file_paths = [self.file_paths[i] for i in batch_indices]\n",
        "        batch_labels = [self.labels[i] for i in batch_indices]\n",
        "\n",
        "        X, y = self.__data_generation(batch_file_paths, batch_labels)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, batch_file_paths, batch_labels):\n",
        "        X = np.empty((len(batch_file_paths), *self.img_size, 1), dtype=np.float32)\n",
        "        y = np.empty((len(batch_file_paths),), dtype=int)\n",
        "\n",
        "        for i, file_path in enumerate(batch_file_paths):\n",
        "            D_dB = np.load(file_path)\n",
        "            D_dB = D_dB[..., np.newaxis]  # Add channel dimension\n",
        "\n",
        "            # Resizing\n",
        "            zoom_factors = [self.img_size[0] / D_dB.shape[0], self.img_size[1] / D_dB.shape[1], 1]\n",
        "            D_dB = zoom(D_dB, zoom_factors, order=3)  # order=3 for cubic interpolation\n",
        "\n",
        "            if self.augment:\n",
        "                if np.random.rand() > 0.5:\n",
        "                    D_dB = np.flip(D_dB, axis=1)  # Flip left-right\n",
        "                if np.random.rand() > 0.5:\n",
        "                    D_dB = np.flip(D_dB, axis=0)  # Flip up-down\n",
        "                if np.random.rand() > 0.5:\n",
        "                    D_dB = D_dB + np.random.uniform(-0.2, 0.2, size=D_dB.shape)  # Random brightness\n",
        "\n",
        "            X[i,] = D_dB\n",
        "            y[i] = batch_labels[i]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "train_generator = OnTheFlyDataGenerator(X_train, y_train, BATCH_SIZE, IMG_SIZE, shuffle=True, augment=True)\n",
        "val_generator = OnTheFlyDataGenerator(X_val, y_val, BATCH_SIZE, IMG_SIZE, shuffle=False, augment=False)\n",
        "\n",
        "# l2 regularization\n",
        "l2_regularizer = tf.keras.regularizers.l2(0.001)\n",
        "\n",
        "model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1), kernel_regularizer=l2_regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2_regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2_regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2_regularizer),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch='500,520')\n",
        "checkpoint_callback = ModelCheckpoint('cry_detection_model.keras', monitor='val_loss', save_best_only=True, mode='min')\n",
        "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "class_weights = {0: 1., 1: 1.}\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[tensorboard_callback, checkpoint_callback, lr_callback, early_stopping_callback]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZHWYittJXK5",
        "outputId": "1119a1d1-078e-4e86-b287-7c663cd424b9"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(val_generator)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f'Accuracy: {acc}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "model.save('cry_detection_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnk1eayLJXK6",
        "outputId": "cf1a0694-a7d7-45bc-9ef4-1eabffe3a87f"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "tflite_models_dir = pathlib.Path(\"tflite_models\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"cry_detection_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_fp16_model = converter.convert()\n",
        "tflite_model_fp16_file = tflite_models_dir/\"cry_detection_model_quant.tflite\"\n",
        "tflite_model_fp16_file.write_bytes(tflite_fp16_model)\n",
        "\n",
        "# converter.target_spec.supported_types = [tf.float16]\n",
        "# tflite_quant_model = converter.convert()\n",
        "# tflite_model_quant_file = tflite_models_dir/\"cry_detection_model_quant_f16.tflite\"\n",
        "# tflite_model_fp16_file.write_bytes(tflite_fp16_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvxu0xjOVKmO",
        "outputId": "19c73f97-5650-430b-afe7-f7378a69180c"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"tflite_models/cry_detection_model_quant.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "def preprocess_audio(file_path, img_size):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    y = librosa.util.normalize(y)\n",
        "    D = librosa.stft(y, n_fft=2048, hop_length=512)\n",
        "    D_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "    # Calculate zoom factors for resizing\n",
        "    zoom_factors = [img_size[0] / D_dB.shape[0], img_size[1] / D_dB.shape[1]]\n",
        "    D_dB_resized = zoom(D_dB, zoom_factors, order=3)  # order=3 for cubic interpolation\n",
        "\n",
        "    # Add channel dimension to match the original function's output\n",
        "    D_dB_resized = D_dB_resized[..., np.newaxis]\n",
        "\n",
        "    return D_dB_resized\n",
        "\n",
        "def predict(file_path, img_size=IMG_SIZE):\n",
        "    input_data = preprocess_audio(file_path, img_size)\n",
        "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
        "\n",
        "    # Set the tensor to point to the input data to be inferred\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    return output_data\n",
        "\n",
        "def process_folder(folder_path, img_size=IMG_SIZE):\n",
        "    correct_predictions = 0\n",
        "    total_files = 0\n",
        "    results = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            prediction = predict(file_path, img_size)\n",
        "            prediction_label = 'Cry' if prediction > 0.5 else 'Not Cry'\n",
        "            results.append((file_name, prediction_label))\n",
        "            ground_truth = 'Cry' if '_cry.wav' in file_name else 'Not Cry'\n",
        "\n",
        "            if prediction_label == ground_truth:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            total_files += 1\n",
        "\n",
        "    accuracy = (correct_predictions / total_files) * 100 if total_files > 0 else 0\n",
        "\n",
        "    return results, accuracy\n",
        "\n",
        "folder_path = '{0}/Test'.format(AUDIO_PATH)\n",
        "predictions, accuracy = process_folder(folder_path)\n",
        "\n",
        "for file_name, prediction_label in predictions:\n",
        "    print(f\"File: {file_name}, Prediction: {prediction_label}\")\n",
        "\n",
        "print(f\"Prediction Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzvX0_TlVKmO",
        "outputId": "d3a3e1bb-c1be-4614-9a6b-09875e712193"
      },
      "outputs": [],
      "source": [
        "lib = ctypes.cdll.LoadLibrary('{0}/libtensorflowlite_c.so'.format(AUDIO_PATH))\n",
        "\n",
        "# Define types for the C API functions\n",
        "lib.TfLiteModelCreate.restype = ctypes.POINTER(ctypes.c_void_p)\n",
        "lib.TfLiteInterpreterCreate.restype = ctypes.POINTER(ctypes.c_void_p)\n",
        "lib.TfLiteInterpreterOptionsCreate.restype = ctypes.POINTER(ctypes.c_void_p)\n",
        "lib.TfLiteInterpreterOptionsSetNumThreads.argtypes = [ctypes.POINTER(ctypes.c_void_p), ctypes.c_int]\n",
        "lib.TfLiteInterpreterOptionsDelete.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\n",
        "lib.TfLiteInterpreterDelete.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\n",
        "lib.TfLiteModelDelete.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\n",
        "lib.TfLiteInterpreterGetInputTensor.restype = ctypes.POINTER(ctypes.c_void_p)\n",
        "lib.TfLiteInterpreterGetOutputTensor.restype = ctypes.POINTER(ctypes.c_void_p)\n",
        "\n",
        "model_path = b\"tflite_models/cry_detection_model_quant.tflite\"\n",
        "with open(model_path, 'rb') as f:\n",
        "    model_data = f.read()\n",
        "\n",
        "model = lib.TfLiteModelCreate(ctypes.c_char_p(model_data), ctypes.c_size_t(len(model_data)))\n",
        "\n",
        "# Create interpreter options and set number of threads\n",
        "options = lib.TfLiteInterpreterOptionsCreate()\n",
        "\n",
        "# Set number of threads (e.g., 2 threads)\n",
        "lib.TfLiteInterpreterOptionsSetNumThreads(options, 2)\n",
        "\n",
        "# Create the interpreter with the custom options\n",
        "interpreter = lib.TfLiteInterpreterCreate(model, options)\n",
        "\n",
        "# Allocate tensors\n",
        "status = lib.TfLiteInterpreterAllocateTensors(interpreter)\n",
        "\n",
        "# Get input and output tensor details\n",
        "input_tensor = lib.TfLiteInterpreterGetInputTensor(interpreter, 0)\n",
        "output_tensor = lib.TfLiteInterpreterGetOutputTensor(interpreter, 0)\n",
        "\n",
        "# def preprocess_audio(file_path, img_size):\n",
        "#     y, sr = librosa.load(file_path, sr=None)\n",
        "#     y = librosa.util.normalize(y)\n",
        "#     D = librosa.stft(y, n_fft=2048, hop_length=512)\n",
        "#     D_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "#     # Rescale the spectrogram to the target img_size\n",
        "#     # zoom_factors = [img_size[0] / D_dB.shape[0], img_size[1] / D_dB.shape[1]]\n",
        "#     # D_dB_resized = zoom(D_dB, zoom_factors).astype(np.float32)\n",
        "\n",
        "#     # Resize using TensorFlow\n",
        "#     # D_dB_resized = tf.image.resize(D_dB[..., np.newaxis], img_size).numpy()\n",
        "#     # D_dB_resized = np.squeeze(D_dB_resized, axis=-1).astype(np.float32)\n",
        "\n",
        "#     # Convert the spectrogram to an image\n",
        "#     D_dB_img = Image.fromarray(D_dB)\n",
        "\n",
        "#     # Resize the image using PIL with LANCZOS resampling\n",
        "#     D_dB_resized = D_dB_img.resize(img_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "#     # Convert back to NumPy array\n",
        "#     D_dB_resized = np.array(D_dB_resized).astype(np.float32)\n",
        "\n",
        "#     return D_dB_resized\n",
        "\n",
        "def preprocess_audio(file_path, img_size):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    y = librosa.util.normalize(y)\n",
        "    D = librosa.stft(y, n_fft=2048, hop_length=512)\n",
        "    D_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "    # Calculate zoom factors for resizing\n",
        "    zoom_factors = [img_size[0] / D_dB.shape[0], img_size[1] / D_dB.shape[1]]\n",
        "    D_dB_resized = zoom(D_dB, zoom_factors, order=3)  # order=3 for cubic interpolation\n",
        "\n",
        "    # Add channel dimension to match the original function's output\n",
        "    D_dB_resized = D_dB_resized[..., np.newaxis]\n",
        "\n",
        "    return D_dB_resized\n",
        "\n",
        "def predict(file_path, img_size=(64, 64)):\n",
        "    input_data = preprocess_audio(file_path, img_size)\n",
        "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
        "\n",
        "    # Set the tensor to point to the input data to be inferred\n",
        "    lib.TfLiteTensorCopyFromBuffer(input_tensor, input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)), ctypes.c_size_t(input_data.nbytes))\n",
        "\n",
        "    # Run inference\n",
        "    lib.TfLiteInterpreterInvoke(interpreter)\n",
        "\n",
        "    # Extract output data\n",
        "    output_size = 1\n",
        "    output_data = np.empty(output_size, dtype=np.float32)\n",
        "    lib.TfLiteTensorCopyToBuffer(output_tensor, output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)), ctypes.c_size_t(output_data.nbytes))\n",
        "\n",
        "    return output_data\n",
        "\n",
        "def process_folder(folder_path, img_size=IMG_SIZE):\n",
        "    correct_predictions = 0\n",
        "    total_files = 0\n",
        "    results = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            prediction = predict(file_path, img_size)\n",
        "            prediction_label = 'Cry' if prediction > 0.5 else 'Not Cry'\n",
        "            results.append((file_name, prediction_label))\n",
        "            ground_truth = 'Cry' if '_cry.wav' in file_name else 'Not Cry'\n",
        "\n",
        "            if prediction_label == ground_truth:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            total_files += 1\n",
        "\n",
        "    accuracy = (correct_predictions / total_files) * 100 if total_files > 0 else 0\n",
        "\n",
        "    return results, accuracy\n",
        "\n",
        "folder_path = '{0}/Test'.format(AUDIO_PATH)\n",
        "predictions, accuracy = process_folder(folder_path)\n",
        "\n",
        "for file_name, prediction_label in predictions:\n",
        "    print(f\"File: {file_name}, Prediction: {prediction_label}\")\n",
        "\n",
        "print(f\"Prediction Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Clean up\n",
        "lib.TfLiteInterpreterDelete(interpreter)\n",
        "lib.TfLiteInterpreterOptionsDelete(options)\n",
        "lib.TfLiteModelDelete(model)\n",
        "\n",
        "print(\"All operations completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "def augment_data(input_folder, output_folder, ogg_files):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Resample ogg files to 22050 Hz\n",
        "    ogg_clips = []\n",
        "    for ogg_file in ogg_files:\n",
        "        y, sr = librosa.load(ogg_file, sr=22050)\n",
        "        if len(y) < 5 * sr:\n",
        "            y = np.tile(y, int(np.ceil(5 * sr / len(y))))[:5 * sr]\n",
        "        else:\n",
        "            y = y[:5 * sr]\n",
        "        ogg_clips.append((y, os.path.basename(ogg_file).split('.')[0]))\n",
        "\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.wav')]\n",
        "    num_groups = len(ogg_files)\n",
        "    files_per_group = len(input_files) // num_groups\n",
        "\n",
        "    # Split input files into groups\n",
        "    for i, ogg_clip in enumerate(ogg_clips):\n",
        "        group_files = input_files[i * files_per_group:(i + 1) * files_per_group]\n",
        "        ogg_clip_data, ogg_clip_name = ogg_clip\n",
        "        \n",
        "        for input_file in group_files:\n",
        "            input_path = os.path.join(input_folder, input_file)\n",
        "            y, sr = librosa.load(input_path, sr=22050)\n",
        "            \n",
        "            # Randomly reduce gain of ogg clip\n",
        "            gain_reduction = random.uniform(0, -20)\n",
        "            ogg_clip_adjusted = librosa.util.normalize(ogg_clip_data) * (10 ** (gain_reduction / 20))\n",
        "            \n",
        "            # Mix the input file with the ogg clip\n",
        "            mixed_audio = y + ogg_clip_adjusted[:len(y)]\n",
        "            mixed_audio = librosa.util.normalize(mixed_audio)\n",
        "            \n",
        "            output_file = f\"{os.path.splitext(input_file)[0]}_{ogg_clip_name}_augmented.wav\"\n",
        "            output_path = os.path.join(output_folder, output_file)\n",
        "            sf.write(output_path, mixed_audio, sr)\n",
        "\n",
        "input_folder = f'{AUDIO_PATH}/cry'\n",
        "output_folder = f'{AUDIO_PATH}/cry/augmented'\n",
        "ogg_files = [f'{AUDIO_PATH}/ac.ogg', f'{AUDIO_PATH}/dishwasher.ogg', f'{AUDIO_PATH}/fan.ogg', f'{AUDIO_PATH}/refridgerator.ogg', \n",
        "             f'{AUDIO_PATH}/tv.ogg',f'{AUDIO_PATH}/vaccum_cleaner.ogg']\n",
        "\n",
        "augment_data(input_folder, output_folder, ogg_files)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
